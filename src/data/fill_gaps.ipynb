{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data\n",
    "train_df = pd.read_csv('../../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../../data/raw/test.csv')\n",
    "\n",
    "# save the length of the train data\n",
    "ntrain = train_df.shape[0]\n",
    "\n",
    "# concatenate the data\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                  0\n",
       "SeriousDlqin2yrs                        37500\n",
       "RevolvingUtilizationOfUnsecuredLines        0\n",
       "age                                         0\n",
       "NumberOfTime30-59DaysPastDueNotWorse        0\n",
       "DebtRatio                                   0\n",
       "MonthlyIncome                           29731\n",
       "NumberOfOpenCreditLinesAndLoans             0\n",
       "NumberOfTimes90DaysLate                     0\n",
       "NumberRealEstateLoansOrLines                0\n",
       "NumberOfTime60-89DaysPastDueNotWorse        0\n",
       "NumberOfDependents                       3924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute = data.copy()\n",
    "\n",
    "# remove the columns\n",
    "columns = ['Unnamed: 0', 'SeriousDlqin2yrs']\n",
    "data_to_impute.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# calculate number of missing values by column\n",
    "missing = data_to_impute.isna().sum() \n",
    "\n",
    "# drop missing values\n",
    "data_to_impute.dropna(inplace=True)\n",
    "\n",
    "# split the data into train and test\n",
    "X_test = data_to_impute.copy().values\n",
    "\n",
    "# create a boolean array of the size of the data\n",
    "mask = np.zeros(X_test.shape, dtype=bool)\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    n_missing = missing.iloc[i]\n",
    "\n",
    "    # randomly select n_missing indexes for the column\n",
    "    idx = np.random.choice(X_test.shape[0], n_missing, replace=False)\n",
    "    mask[idx, i] = True\n",
    "\n",
    "# create the train data\n",
    "X_train = np.copy(X_test)\n",
    "X_train[mask] = np.nan\n",
    "\n",
    "del data_to_impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the data with the simple imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train.copy())\n",
    "\n",
    "# create a dataframe to store the metrics\n",
    "metrics = pd.DataFrame(columns=['mae', 'mse'])\n",
    "\n",
    "# calculate mae and mse for the imputed data\n",
    "mae = np.mean(np.abs(X_test[mask] - X_train_imputed[mask]))\n",
    "mse = np.mean((X_test[mask] - X_train_imputed[mask])**2)\n",
    "\n",
    "metrics.loc['SimpleImputer'] = [mae, mse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # try load the iterative imputer with pickle\n",
    "    with open('../../models/impute/iterative_imputer.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "except:\n",
    "    # impute the data with the iterative imputer\n",
    "    imputer = IterativeImputer(max_iter=1000, random_state=0)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/iterative_imputer.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_train_imputed = imputer.transform(X_train.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data\n",
    "mae = np.mean(np.abs(X_test[mask] - X_train_imputed[mask]))\n",
    "mse = np.mean((X_test[mask] - X_train_imputed[mask])**2)\n",
    "\n",
    "metrics.loc['IterativeImputer'] = [mae, mse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # try load the knn imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "except:\n",
    "    # impute the data with the knn imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.copy())\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train)\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_train_imputed = imputer.transform(X_train.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data\n",
    "mae = np.mean(np.abs(X_test[mask] - X_train_imputed[mask]))\n",
    "mse = np.mean((X_test[mask] - X_train_imputed[mask])**2)\n",
    "\n",
    "metrics.loc['KNNImputer'] = [mae, mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          mae           mse\n",
      "SimpleImputer     3050.222905  1.009808e+08\n",
      "IterativeImputer  2779.194077  9.781320e+07\n",
      "KNNImputer        2918.852035  1.436448e+08\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices of the columns with missing values\n",
    "missing_idx = np.where(np.sum(mask, axis=0) > 0)[0]\n",
    "\n",
    "def tune_xgboost(X, y, space, scoring='mse', n_estimators=500, balance=False):\n",
    "    \n",
    "    # define evaluation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "    # define the model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, device='cuda')\n",
    "\n",
    "    # define search\n",
    "    search = RandomizedSearchCV(model, space, scoring=scoring, n_jobs=-1, cv=cv, n_iter=50)\n",
    "    \n",
    "    # execute search\n",
    "    result = search.fit(X, y)\n",
    "    \n",
    "    # plot results\n",
    "    results_df = pd.DataFrame(result.cv_results_)\n",
    "    for key, values in space.items():\n",
    "        \n",
    "        # group the results by the hyperparameter\n",
    "        param_means = []\n",
    "        param_stds = []\n",
    "        for value in values:\n",
    "            mask = results_df['param_' + key] == value\n",
    "            param_means.append(np.mean(results_df[mask]['mean_test_score']))\n",
    "            param_stds.append(np.std(results_df[mask]['mean_test_score']))\n",
    "\n",
    "        # create plot with two subplots side by side\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        fig.suptitle(key)\n",
    "        ax[0].plot(values, param_means)\n",
    "        ax[0].set_title('Mean test scores')\n",
    "        ax[0].set_xlabel(key)\n",
    "        ax[0].set_ylabel('mean scores')\n",
    "        padding = 0.1\n",
    "        ax[0].set_ylim(max(0, min(param_means) - padding), min(1, max(param_means) + padding))\n",
    "\n",
    "        ax[1].plot(values, param_stds)\n",
    "        ax[1].set_title('Mean score std')\n",
    "        ax[1].set_xlabel(key)\n",
    "        ax[1].set_ylabel('score std')\n",
    "        padding = 0.05\n",
    "        ax[1].set_ylim(max(0, min(param_stds) - padding), min(1, max(param_stds) + padding))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # summarize result\n",
    "    print('Best Score: %s' % result.best_score_)\n",
    "    print('Best Hyperparameters:')\n",
    "    for k, v in result.best_params_.items():\n",
    "        print('%s: %s' % (k, v))\n",
    "\n",
    "    # best model\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result.best_params_, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 9]\n"
     ]
    }
   ],
   "source": [
    "# find indices of the columns with missing values\n",
    "missing_idx = np.where(np.sum(mask, axis=0) > 0)[0]\n",
    "\n",
    "# define the space of hyperparameters\n",
    "space = {\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'gamma': [0, 0.5, 2, 5],\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.05],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.05],\n",
    "    'n_estimators': [500]\n",
    "}\n",
    "\n",
    "for idx in missing_idx:\n",
    "    X = np.copy(X_train)\n",
    "    X = np.delete(X, idx, axis=1)\n",
    "    y = np.copy(X_train_imputed[:, idx])\n",
    "\n",
    "    try:\n",
    "        # try load the model with pickle\n",
    "        with open(f'../../models/impute/xgboost_imputer_{idx}.pkl', 'rb') as f:\n",
    "            model = pkl.load(f)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_with_miss(a, b, l=0.0):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two arrays, taking into account missing values.\n",
    "\n",
    "    Parameters:\n",
    "    a (array-like): The first array.\n",
    "    b (array-like): The second array.\n",
    "    l (float, optional): The penalty for missing values. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "    float: The distance between the two arrays.\n",
    "\n",
    "    Raises:\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    if len(a) != len(b):\n",
    "        return np.inf\n",
    "    ls = l * np.ones(len(a))\n",
    "    msk = ~ (np.isnan(a) | np.isnan(b))\n",
    "    res = np.sum((np.abs(a - b)[msk])) + np.sum((ls[~msk]))\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

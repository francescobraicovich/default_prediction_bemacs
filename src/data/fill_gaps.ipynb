{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from missforest.missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data\n",
    "train_df = pd.read_csv('../../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../../data/raw/test.csv')\n",
    "\n",
    "# save the length of the train data\n",
    "ntrain = train_df.shape[0]\n",
    "\n",
    "# concatenate the data\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>7959.688894</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165166</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>2851.722407</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642979</td>\n",
       "      <td>1115.657341</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3603.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0           0               0.0                              0.034949  59.0   \n",
       "1           1               0.0                              0.155308  47.0   \n",
       "2           2               0.0                              0.165166  62.0   \n",
       "3           3               0.0                              0.010886  61.0   \n",
       "4           4               0.0                              0.000717  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                   0.0     0.004933    7959.688894   \n",
       "1                                   0.0   881.000000            NaN   \n",
       "2                                   1.0     0.020327    2851.722407   \n",
       "3                                   0.0     0.642979    1115.657341   \n",
       "4                                   0.0  3603.000000            NaN   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                              5.0                      0.0   \n",
       "1                              6.0                      0.0   \n",
       "2                              8.0                      0.0   \n",
       "3                              6.0                      0.0   \n",
       "4                             15.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           0.0                                   0.0   \n",
       "1                           1.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           1.0                                   0.0   \n",
       "4                           3.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                  0\n",
       "SeriousDlqin2yrs                        37500\n",
       "RevolvingUtilizationOfUnsecuredLines        0\n",
       "age                                         0\n",
       "NumberOfTime30-59DaysPastDueNotWorse        0\n",
       "DebtRatio                                   0\n",
       "MonthlyIncome                           29731\n",
       "NumberOfOpenCreditLinesAndLoans             0\n",
       "NumberOfTimes90DaysLate                     0\n",
       "NumberRealEstateLoansOrLines                0\n",
       "NumberOfTime60-89DaysPastDueNotWorse        0\n",
       "NumberOfDependents                       3924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                              112500\n",
       "SeriousDlqin2yrs                             2\n",
       "RevolvingUtilizationOfUnsecuredLines    125728\n",
       "age                                         89\n",
       "NumberOfTime30-59DaysPastDueNotWorse        16\n",
       "DebtRatio                               114194\n",
       "MonthlyIncome                           118636\n",
       "NumberOfOpenCreditLinesAndLoans             58\n",
       "NumberOfTimes90DaysLate                     19\n",
       "NumberRealEstateLoansOrLines                28\n",
       "NumberOfTime60-89DaysPastDueNotWorse        13\n",
       "NumberOfDependents                          13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the unique values for each column\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (96215, 10)\n",
      "X_test shape: (24054, 10)\n"
     ]
    }
   ],
   "source": [
    "data_to_impute = data.copy()\n",
    "\n",
    "# remove the columns\n",
    "columns_to_drop = ['Unnamed: 0', 'SeriousDlqin2yrs']\n",
    "data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# calculate number of missing values by column\n",
    "missing = data_to_impute.isna().sum() \n",
    "\n",
    "# drop missing values\n",
    "data_to_impute.dropna(inplace=True)\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test = train_test_split(data_to_impute, test_size=0.2, random_state=42)\n",
    "\n",
    "# save the indexes\n",
    "train_idx = X_train.index\n",
    "test_idx = X_test.index\n",
    "\n",
    "# assert there are no missing values in the train set\n",
    "assert X_train.isna().sum().sum() == 0\n",
    "\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "y = np.copy(X_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "# create a boolean array of the size of the data\n",
    "mask = np.zeros(X_test.shape, dtype=bool)\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    n_missing = missing.iloc[i]\n",
    "    prc_missing = 0.85 if n_missing != 0 else 0 # percentage of missing values\n",
    "    n_missing = int(prc_missing * X_test.shape[0])\n",
    "\n",
    "    # randomly select n_missing indexes for the column\n",
    "    idx = np.random.choice(X_test.shape[0], n_missing, replace=False)\n",
    "    mask[idx, i] = True\n",
    "\n",
    "# create the missing values\n",
    "X_test[mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(metrics_df, missing_idx, X_test_imputed, y, mask, model_name):\n",
    "    for i in missing_idx:\n",
    "        mae = np.mean(np.abs(X_test_imputed[mask[:, i], i] - y[mask[:, i], i]))\n",
    "        mse = np.mean((X_test_imputed[mask[:, i], i] - y[mask[:, i], i])**2)\n",
    "\n",
    "        metrics_df.loc[model_name, f'mae_{i}'] = mae\n",
    "        metrics_df.loc[model_name, f'mse_{i}'] = mse\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices of the columns with missing values\n",
    "missing_idx = np.where(np.sum(mask, axis=0) > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     mae_4     mae_9           mse_4     mse_9\n",
      "SimpleImputer  3354.265708  0.934392  89916915.96173  1.333787\n"
     ]
    }
   ],
   "source": [
    "# impute the data with the simple imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(X_train.copy())\n",
    "\n",
    "# create a dataframe to store the metrics\n",
    "columns_of_df = [f'mae_{i}' for i in missing_idx] + [f'mse_{i}' for i in missing_idx]\n",
    "metrics = pd.DataFrame(columns=columns_of_df)\n",
    "\n",
    "# impute the data\n",
    "X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'SimpleImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392   89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084  86646251.872343  1.256595\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try load the iterative imputer with pickle\n",
    "    with open('../../models/impute/iterative_imputer.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "except:\n",
    "    # impute the data with the iterative imputer\n",
    "    imputer = IterativeImputer(max_iter=1000, random_state=0)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/iterative_imputer.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'IterativeImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392   89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084  86646251.872343  1.256595\n",
      "KNNImputer        2462.245568  0.727669  88009527.237681  1.127556\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try load the knn imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "    # try load the imputed data\n",
    "    with open('../../data/processed/impute/knn_imputed.pkl', 'rb') as f:\n",
    "        X_test_imputed = pkl.load(f)\n",
    "except:\n",
    "    # impute the data with the knn imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # impute the data\n",
    "    X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "    # save the imputed data\n",
    "    with open('../../data/processed/impute/knn_imputed.pkl', 'wb') as f:\n",
    "        pkl.dump(X_test_imputed, f)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'KNNImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392   89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084  86646251.872343  1.256595\n",
      "KNNImputer        2462.245568  0.727669  88009527.237681  1.127556\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgboost(X, y, space, scoring, n_estimators=250, n_iter=50):\n",
    "    \n",
    "    # define evaluation\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "    # define the model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, device='cuda')\n",
    "\n",
    "    # define search\n",
    "    search = RandomizedSearchCV(model, space, scoring=scoring, n_jobs=-1, cv=cv, n_iter=n_iter, random_state=0)\n",
    "    \n",
    "    # execute search\n",
    "    result = search.fit(X, y)\n",
    "    \n",
    "    # plot results\n",
    "    results_df = pd.DataFrame(result.cv_results_)\n",
    "    for key, values in space.items():\n",
    "        \n",
    "        # group the results by the hyperparameter\n",
    "        param_means = []\n",
    "        param_stds = []\n",
    "        for value in values:\n",
    "            mask = results_df['param_' + key] == value\n",
    "            param_means.append(np.mean(results_df[mask]['mean_test_score']))\n",
    "            param_stds.append(np.std(results_df[mask]['mean_test_score']))\n",
    "        \"\"\"\n",
    "        # create plot with two subplots side by side\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        fig.suptitle(key)\n",
    "        ax[0].plot(values, param_means)\n",
    "        ax[0].set_title('Mean test scores')\n",
    "        ax[0].set_xlabel(key)\n",
    "        ax[0].set_ylabel('mean scores')\n",
    "        padding = 0.1\n",
    "        ax[0].set_ylim(max(0, min(param_means) - padding), min(1, max(param_means) + padding))\n",
    "\n",
    "        ax[1].plot(values, param_stds)\n",
    "        ax[1].set_title('Mean score std')\n",
    "        ax[1].set_xlabel(key)\n",
    "        ax[1].set_ylabel('score std')\n",
    "        padding = 0.05\n",
    "        ax[1].set_ylim(max(0, min(param_stds) - padding), min(1, max(param_stds) + padding))\n",
    "\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    # summarize result\n",
    "    print('Best Score: %s' % result.best_score_)\n",
    "    print('Best Hyperparameters:')\n",
    "    for k, v in result.best_params_.items():\n",
    "        print('%s: %s' % (k, v))\n",
    "\n",
    "    # best model\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result.best_params_, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing column 4...\n",
      "\n",
      "Imputing column 9...\n",
      "\n",
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392    89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084   86646251.872343  1.256595\n",
      "KNNImputer        2462.245568  0.727669   88009527.237681  1.127556\n",
      "XGBoost           6264.086579  0.949105  327707353.272173  1.386946\n"
     ]
    }
   ],
   "source": [
    "# find indices of the columns with missing values\n",
    "missing_idx = np.where(np.sum(mask, axis=0) > 0)[0]\n",
    "\n",
    "# define the space of hyperparameters\n",
    "space = {\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'gamma': [0, 0.5, 2, 5],\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.05],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "X_test_imputed = np.copy(X_test)\n",
    "\n",
    "for idx in missing_idx:\n",
    "    print(f'Imputing column {idx}...')\n",
    "    X_train_xgboost = np.copy(X_train)\n",
    "    X_train_xgboost = np.delete(X_train, idx, axis=1)\n",
    "    y_train_xgboost = np.copy(X_train[:, idx])\n",
    "\n",
    "    X_test_xgboost = np.copy(X_test)\n",
    "    X_test_xgboost = np.delete(X_test, idx, axis=1)\n",
    "    y_test_xgboost = np.copy(X_test[:, idx])\n",
    "\n",
    "    # find the the indices of the missing values in the column\n",
    "    nan_mask = np.isnan(y_test_xgboost)\n",
    "\n",
    "    try:\n",
    "        # try load the model with pickle\n",
    "        with open(f'../../models/impute/xgboost_{idx}.pkl', 'rb') as f:\n",
    "            best_model = pkl.load(f)\n",
    "    except:\n",
    "        # tune the model\n",
    "        best_params, model = tune_xgboost(X_train_xgboost, y_train_xgboost, space, scoring='neg_mean_squared_error', n_estimators=100, n_iter=50)\n",
    "\n",
    "        # define new number of estimators\n",
    "        n_estimators = 2000\n",
    "\n",
    "        # add the number of estimators to the best params\n",
    "        best_params['n_estimators'] = n_estimators\n",
    "        best_params['objective'] = 'reg:squarederror'\n",
    "\n",
    "        # fit a new model with the best parameters\n",
    "        best_model = XGBRegressor(**best_params)\n",
    "\n",
    "        # fit the model\n",
    "        best_model.fit(X_train_xgboost, y_train_xgboost)\n",
    "\n",
    "        # save the model with pickle\n",
    "        with open(f'../../models/impute/xgboost_{idx}.pkl', 'wb') as f:\n",
    "            pkl.dump(best_model, f)\n",
    "\n",
    "    # impute the data with the xgboost imputer\n",
    "    y_pred = best_model.predict(X_test_xgboost)\n",
    "\n",
    "    # replace the missing values\n",
    "    X_test_imputed[nan_mask, idx] = y_pred[nan_mask]\n",
    "\n",
    "    print('')\n",
    "\n",
    "# for each column in the missing_idx calculate the mae and mse\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'XGBoost')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Personale/default_prediction_bemacs/myenv/lib/python3.12/site-packages/missforest/missforest.py:227: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  x[c].fillna(initial_imputations[c], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392    89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084   86646251.872343  1.256595\n",
      "KNNImputer        2462.245568  0.727669   88009527.237681  1.127556\n",
      "XGBoost           6264.086579  0.949105  327707353.272173  1.386946\n",
      "MissForest        2658.560538  0.855797   73635980.364331  1.230534\n"
     ]
    }
   ],
   "source": [
    "# try the miss forest imputer\n",
    "\n",
    "try:\n",
    "    # try load the miss forest imputer with pickle\n",
    "    with open('../../models/impute/miss_forest.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "except:\n",
    "    # impute the data with the miss forest imputer\n",
    "    clf = RandomForestClassifier(n_jobs=-1)\n",
    "    rgr = RandomForestRegressor(n_jobs=-1)\n",
    "    imputer = MissForest(clf, rgr, max_iter=1000000)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/miss_forest.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "X_test_imputed = imputer.transform(X_test_df.copy())\n",
    "X_test_imputed = X_test_imputed.values\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'MissForest')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the columns of the metrics dataframe\n",
    "scaler = StandardScaler()\n",
    "metrics_scaled = scaler.fit_transform(metrics)\n",
    "\n",
    "# sum along each row to get the total error\n",
    "metrics_scaled = np.sum(metrics_scaled, axis=1)\n",
    "\n",
    "# find the indices of the n best imputers\n",
    "n = 3\n",
    "best_imputers_idx = metrics_scaled.argsort()[:n]\n",
    "\n",
    "# find the names of the best imputers\n",
    "best_imputers = metrics.index[best_imputers_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3354.265708  0.934392    89916915.96173  1.333787\n",
      "IterativeImputer  3011.610579  0.890084   86646251.872343  1.256595\n",
      "KNNImputer        2462.245568  0.727669   88009527.237681  1.127556\n",
      "XGBoost           6264.086579  0.949105  327707353.272173  1.386946\n",
      "MissForest        2658.560538  0.855797   73635980.364331  1.230534\n",
      "Ensemble          2264.025476  0.720802   79022407.070898  0.993948\n"
     ]
    }
   ],
   "source": [
    "imputers_dict = {\n",
    "    'SimpleImputer': SimpleImputer(strategy='mean'),\n",
    "    'IterativeImputer': IterativeImputer(max_iter=1000, random_state=0),\n",
    "    'KNNImputer': KNNImputer(n_neighbors=5),\n",
    "    'MissForest': MissForest(clf, rgr, max_iter=1000000)\n",
    "}\n",
    "\n",
    "try:\n",
    "    # try load the ensembled imputed data\n",
    "    with open('../../data/processed/impute/ensemble_imputed.pkl', 'rb') as f:\n",
    "        X_test_imputed = pkl.load(f)\n",
    "except:\n",
    "    X_test_imputed = np.copy(X_test)\n",
    "    mask = np.isnan(X_test)\n",
    "    X_test_imputed[mask] = 0\n",
    "\n",
    "    for imputer_name in best_imputers:\n",
    "        print(f'Imputing with {imputer_name}...')\n",
    "        \n",
    "        imputer = imputers_dict[imputer_name]\n",
    "\n",
    "        # fit the imputer\n",
    "        imputer.fit(X_train.copy())\n",
    "        print('Imputer fitted...')\n",
    "\n",
    "        # impute the data\n",
    "        try:\n",
    "            data_to_add = imputer.transform(np.copy(X_test))\n",
    "        except:\n",
    "            data_to_add = imputer.transform(pd.DataFrame(np.copy(X_test)))\n",
    "            data_to_add = data_to_add.values\n",
    "        print('Data imputed...')\n",
    "\n",
    "        # add the imputed data to the imputed_X_test\n",
    "        X_test_imputed[mask] += data_to_add[mask]\n",
    "        print('')\n",
    "\n",
    "    # average the imputed data\n",
    "    X_test_imputed[mask] /= n\n",
    "\n",
    "    # save the imputed data as a pickle\n",
    "    with open('../../data/processed/impute/ensemble_imputed.pkl', 'wb') as f:\n",
    "        pkl.dump(X_test_imputed, f)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'Ensemble')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing with KNNImputer...\n",
      "Imputer fitted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Personale/default_prediction_bemacs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but KNNImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imputed...\n",
      "\n",
      "Imputing with MissForest...\n",
      "Imputer fitted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Personale/default_prediction_bemacs/myenv/lib/python3.12/site-packages/missforest/missforest.py:227: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  x[c].fillna(initial_imputations[c], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imputed...\n",
      "\n",
      "Imputing with IterativeImputer...\n",
      "Imputer fitted...\n",
      "Data imputed...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Personale/default_prediction_bemacs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but IterativeImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try read the data\n",
    "    train_df = pd.read_csv('../../data/proessed/train_imputed.csv')\n",
    "    test_df = pd.read_csv('../../data/processed/test_imputed.csv')\n",
    "except:\n",
    "    data_to_impute = data.copy()\n",
    "\n",
    "    # remove the columns\n",
    "    data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    mask = data_to_impute.isna()\n",
    "\n",
    "    imputed_data = np.copy(data_to_impute.values)\n",
    "\n",
    "    # set all values to zero\n",
    "    imputed_data[mask] = 0\n",
    "\n",
    "    # impute the data with the best imputers\n",
    "    for imputer_name in best_imputers:\n",
    "        print(f'Imputing with {imputer_name}...')\n",
    "        \n",
    "        imputer = imputers_dict[imputer_name]\n",
    "\n",
    "        # fit the imputer\n",
    "        imputer.fit(data_to_impute.copy())\n",
    "        print('Imputer fitted...')\n",
    "\n",
    "        # impute the data\n",
    "        try:\n",
    "            data_to_add = imputer.transform(data_to_impute.copy())\n",
    "            data_to_add = data_to_add.values\n",
    "        except:\n",
    "            data_to_add = imputer.transform(data_to_impute.copy().values)\n",
    "        print('Data imputed...')\n",
    "\n",
    "        # add the imputed data to the imputed_X_test\n",
    "        imputed_data[mask] += data_to_add[mask]\n",
    "        print('')\n",
    "\n",
    "    # average the imputed data\n",
    "    imputed_data[mask] /= n\n",
    "\n",
    "    # make the imputed data a dataframe with the same columns as the original data\n",
    "    imputed_data = pd.DataFrame(imputed_data, columns=data_to_impute.columns)\n",
    "\n",
    "    # add back the columns that were dropped\n",
    "    imputed_data['Unnamed: 0'] = data['Unnamed: 0']\n",
    "    imputed_data['SeriousDlqin2yrs'] = data['SeriousDlqin2yrs']\n",
    "\n",
    "    # split back into train and test\n",
    "    train_df = imputed_data.iloc[:ntrain, :]\n",
    "    test_df = imputed_data.iloc[ntrain:, :]\n",
    "\n",
    "    # save the data\n",
    "    train_df.to_csv('../../data/processed/train_imputed.csv', index=False)\n",
    "    test_df.to_csv('../../data/processed/test_imputed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Personale/default_prediction_bemacs/src/models\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from missforest.missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from impute import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data\n",
    "train_df = pd.read_csv('../../../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../../../data/raw/test.csv')\n",
    "\n",
    "# save the length of the train data\n",
    "ntrain = train_df.shape[0]\n",
    "\n",
    "# concatenate the data\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>7959.688894</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165166</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>2851.722407</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642979</td>\n",
       "      <td>1115.657341</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3603.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0           0               0.0                              0.034949  59.0   \n",
       "1           1               0.0                              0.155308  47.0   \n",
       "2           2               0.0                              0.165166  62.0   \n",
       "3           3               0.0                              0.010886  61.0   \n",
       "4           4               0.0                              0.000717  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                   0.0     0.004933    7959.688894   \n",
       "1                                   0.0   881.000000            NaN   \n",
       "2                                   1.0     0.020327    2851.722407   \n",
       "3                                   0.0     0.642979    1115.657341   \n",
       "4                                   0.0  3603.000000            NaN   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                              5.0                      0.0   \n",
       "1                              6.0                      0.0   \n",
       "2                              8.0                      0.0   \n",
       "3                              6.0                      0.0   \n",
       "4                             15.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           0.0                                   0.0   \n",
       "1                           1.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           1.0                                   0.0   \n",
       "4                           3.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                  0\n",
       "SeriousDlqin2yrs                        37500\n",
       "RevolvingUtilizationOfUnsecuredLines        0\n",
       "age                                         0\n",
       "NumberOfTime30-59DaysPastDueNotWorse        0\n",
       "DebtRatio                                   0\n",
       "MonthlyIncome                           29731\n",
       "NumberOfOpenCreditLinesAndLoans             0\n",
       "NumberOfTimes90DaysLate                     0\n",
       "NumberRealEstateLoansOrLines                0\n",
       "NumberOfTime60-89DaysPastDueNotWorse        0\n",
       "NumberOfDependents                       3924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                              112500\n",
       "SeriousDlqin2yrs                             2\n",
       "RevolvingUtilizationOfUnsecuredLines    125728\n",
       "age                                         89\n",
       "NumberOfTime30-59DaysPastDueNotWorse        16\n",
       "DebtRatio                               114194\n",
       "MonthlyIncome                           118636\n",
       "NumberOfOpenCreditLinesAndLoans             58\n",
       "NumberOfTimes90DaysLate                     19\n",
       "NumberRealEstateLoansOrLines                28\n",
       "NumberOfTime60-89DaysPastDueNotWorse        13\n",
       "NumberOfDependents                          13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the unique values for each column\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute = data.copy()\n",
    "\n",
    "# remove the columns\n",
    "columns_to_drop = ['Unnamed: 0', 'SeriousDlqin2yrs']\n",
    "data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "length_of_dataset = data_to_impute.shape[0]\n",
    "\n",
    "# calculate number of missing values by column\n",
    "missing_values_by_column = data_to_impute.isna().sum()\n",
    "prc_missing_values_by_column = missing_values_by_column / length_of_dataset\n",
    "\n",
    "# drop missing values\n",
    "data_to_impute.dropna(inplace=True)\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test = train_test_split(data_to_impute, test_size=0.2, random_state=0) # 80% train, 20% test both with no missing values\n",
    "X_train, X_test = X_train.values, X_test.values # convert to numpy arrays\n",
    "y_train, y_test = np.copy(X_train), np.copy(X_test) # copy the test data to use as the ground truth\n",
    "\n",
    "# assert there are np missing values in any of the datasets\n",
    "assert np.isnan(X_train).sum() == 0\n",
    "assert np.isnan(X_test).sum() == 0\n",
    "assert np.isnan(y_train).sum() == 0\n",
    "assert np.isnan(y_test).sum() == 0\n",
    "\n",
    "del data_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_values(X, prc_missing_values_by_column, seed=0):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    # create a boolean array of the size of the data\n",
    "    missing_value_mask = np.zeros(X.shape, dtype=bool)\n",
    "\n",
    "    for i in range(X_test.shape[1]):\n",
    "        prc_missing = prc_missing_values_by_column[i]\n",
    "\n",
    "        # calculate how many missing values to create\n",
    "        n_to_nan = int(prc_missing * X_test.shape[0])\n",
    "\n",
    "        # randomly select n_missing indexes for the column\n",
    "        idx = np.random.choice(X_test.shape[0], n_to_nan, replace=False)\n",
    "        missing_value_mask[idx, i] = True\n",
    "\n",
    "    # create the missing values\n",
    "    X[missing_value_mask] = np.nan\n",
    "\n",
    "    return X, missing_value_mask\n",
    "\n",
    "# create missing values in the train data\n",
    "X_train, missing_value_mask_train = create_missing_values(X_train.copy(), prc_missing_values_by_column)\n",
    "\n",
    "# create missing values in the test data\n",
    "X_test, missing_value_mask_test = create_missing_values(X_test.copy(), prc_missing_values_by_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(metrics_df, X_test_imputed, y, mask, model_name):\n",
    "    \"\"\"\n",
    "    Update the metrics dataframe with Mean Absolute Error (MAE) and Mean Squared Error (MSE) for each missing index.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_df (pandas.DataFrame): The metrics dataframe to update.\n",
    "    - X_test_imputed (numpy.ndarray): The imputed test data.\n",
    "    - y (numpy.ndarray): The ground truth test data.\n",
    "    - mask (numpy.ndarray): The mask indicating missing values.\n",
    "    - model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df (pandas.DataFrame): The updated metrics dataframe.\n",
    "    \"\"\"\n",
    "    missing_idx = np.unique(np.where(mask)[1])\n",
    "    for i in missing_idx:\n",
    "        mae = np.mean(np.abs(X_test_imputed[mask[:, i], i] - y[mask[:, i], i]))\n",
    "        mse = np.mean((X_test_imputed[mask[:, i], i] - y[mask[:, i], i])**2)\n",
    "\n",
    "        metrics_df.loc[model_name, f'mae_{i}'] = mae\n",
    "        metrics_df.loc[model_name, f'mse_{i}'] = mse\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     mae_4     mae_9          mse_4     mse_9\n",
      "SimpleImputer  3307.973839  0.956714  27552228.6627  1.409101\n"
     ]
    }
   ],
   "source": [
    "# impute the data with the simple imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(X_train.copy())\n",
    "\n",
    "# create a dataframe to store the metrics\n",
    "missing_idx = np.unique(np.where(missing_value_mask_test)[1])\n",
    "columns_of_df = [f'mae_{i}' for i in missing_idx] + [f'mse_{i}' for i in missing_idx]\n",
    "metrics = pd.DataFrame(columns=columns_of_df)\n",
    "\n",
    "# impute the data\n",
    "X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'SimpleImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_prc_mae(estimator, X, y, parameters):\n",
    "\n",
    "    y_pred = estimator.set_params(**parameters).transform(X.copy())\n",
    "\n",
    "    negative_value_mask = y_pred < 0\n",
    "    sum_of_negative_values = np.sum(y_pred[negative_value_mask])\n",
    "\n",
    "    # calculate the mae for each column\n",
    "    mae = np.mean((y_pred - y))\n",
    "\n",
    "    # calculate prc_mae for each column\n",
    "    prc_mae = mae / np.mean(np.abs(y))\n",
    "\n",
    "    # calculate r squared\n",
    "    r_squared = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "    # sum the prc_mae for each column\n",
    "    prc_mae_sum = np.sum(prc_mae)\n",
    "\n",
    "    return - prc_mae_sum + r_squared + sum_of_negative_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the imputer...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "Best Score: \u001b[1m0.6372152976490068\u001b[0m\n",
      "Best Hyperparameters:\n",
      "initial_strategy: mean\n",
      "max_iter: 25\n",
      "n_nearest_features: 5\n",
      "\n",
      "Fine tuning completed.\n",
      "Imputer saved.\n",
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3307.973839  0.956714    27552228.6627  1.409101\n",
      "IterativeImputer  2952.872716  0.906348  24813904.064018  1.302942\n"
     ]
    }
   ],
   "source": [
    "# define the path to save the model\n",
    "path_to_save = '../../../models/impute/iterative_imputer.pkl'\n",
    "\n",
    "model = IterativeImputer(random_state=0)\n",
    "\n",
    "parameter_space = {\n",
    "    'max_iter': [25, 50, 75],\n",
    "    'n_nearest_features': [5, 10, 15],\n",
    "    'initial_strategy': ['mean', 'median', 'most_frequent'],\n",
    "}\n",
    "\n",
    "retrain = True\n",
    "\n",
    "# scoring function, additional variables added only for compatibility with the scoring function\n",
    "scoring = lambda estimator, X, y, **param: neg_prc_mae(estimator, X, y, param)\n",
    "\n",
    "# impute the data with the iterative imputer\n",
    "X_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\n",
    "                        param_space=parameter_space, scoring=scoring, \n",
    "                        path_to_save=path_to_save, normalize=False, retrain_if_exists=retrain)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'IterativeImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the imputer...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Best Score: \u001b[1m0.6530581891447848\u001b[0m\n",
      "Best Hyperparameters:\n",
      "metric: nan_euclidean\n",
      "n_neighbors: 7\n",
      "weights: distance\n",
      "\n",
      "Fine tuning completed.\n",
      "Imputer saved.\n",
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3307.973839  0.956714    27552228.6627  1.409101\n",
      "IterativeImputer  2952.872716  0.906348  24813904.064018  1.302942\n",
      "KNNImputer        2863.905205   0.88298  25430505.059991  1.429329\n"
     ]
    }
   ],
   "source": [
    "# impute the data with knn imputer\n",
    "path = '../../../models/impute/knn_imputer.pkl'\n",
    "path_data = '../../../data/processed/impute_test/knn_imputed.pkl'\n",
    "\n",
    "model = KNNImputer()\n",
    "\n",
    "parameter_space = {\n",
    "    'n_neighbors': [3, 5, 6,  7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['nan_euclidean'],\n",
    "}\n",
    "\n",
    "# scoring function, additional variables added only for compatibility with the scoring function\n",
    "scoring = lambda estimator, X, y, **param: neg_prc_mae(estimator, X, y.copy(), param)\n",
    "\n",
    "retrain = True\n",
    "\n",
    "# impute the data with the knn imputer\n",
    "X_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\n",
    "                        param_space=parameter_space, scoring=scoring, \n",
    "                        path_to_save=path, path_data=path_data, normalize=False, retrain_if_exists=retrain)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'KNNImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on column: 4\n",
      "Fine tuning the imputer...\n",
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "\n",
      "Best Score: \u001b[1m-170787283.44281635\u001b[0m\n",
      "Best Hyperparameters:\n",
      "colsample_bylevel: 0.6\n",
      "colsample_bytree: 1\n",
      "learning_rate: 0.1\n",
      "max_depth: 3\n",
      "n_estimators: 100\n",
      "subsample: 1\n",
      "\n",
      "Fine tuning completed.\n",
      "Imputer saved.\n",
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "Working on column: 9\n",
      "Fine tuning the imputer...\n",
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "\n",
      "Best Score: \u001b[1m-1.0601627292866216\u001b[0m\n",
      "Best Hyperparameters:\n",
      "colsample_bylevel: 0.6\n",
      "colsample_bytree: 1\n",
      "learning_rate: 0.1\n",
      "max_depth: 5\n",
      "n_estimators: 100\n",
      "subsample: 0.8\n",
      "\n",
      "Fine tuning completed.\n",
      "Imputer saved.\n",
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3307.973839  0.956714     27552228.6627  1.409101\n",
      "IterativeImputer  2952.872716  0.906348   24813904.064018  1.302942\n",
      "KNNImputer        2863.905205   0.88298   25430505.059991  1.429329\n",
      "XGBRegressor      2614.571848  0.801925  215684985.646753  1.116717\n"
     ]
    }
   ],
   "source": [
    "path = '../../../models/impute/xgb_imputer'\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "parameter_space = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1],\n",
    "    'colsample_bytree': [0.6, 0.8, 1],\n",
    "    'colsample_bylevel': [0.6, 0.8, 1],\n",
    "}\n",
    "\n",
    "retrain = True\n",
    "\n",
    "# define new train and test data because xgboost is not an imputer\n",
    "X_train_xgb = y_train.copy()\n",
    "X_test_xgb = y_test.copy()\n",
    "\n",
    "X_test_imputed = y_test.copy()\n",
    "\n",
    "scoring = lambda y_pred, y_true: np.mean((y_pred - y_true)**2) \n",
    "\n",
    "for idx in missing_idx:\n",
    "    print('Working on column:', idx)\n",
    "    new_path = path + f'_{idx}.pkl'\n",
    "\n",
    "    # select the column to predict\n",
    "    y_train_xgb = X_train_xgb[:, idx]\n",
    "\n",
    "    # drop the column to predict\n",
    "    X_train_xgb_i = np.delete(np.copy(X_train_xgb), idx, axis=1)\n",
    "    X_test_xgb_i = np.delete(np.copy(X_test_xgb), idx, axis=1)\n",
    "    y_train_xgb_i = np.copy(X_train_xgb[:, idx])\n",
    "\n",
    "    # train the model\n",
    "    col_to_insert = impute(X_train_xgb_i, X_test_xgb_i, y_train_xgb_i, model, \n",
    "                           param_space=parameter_space, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           path_to_save=new_path, normalize=False, \n",
    "                           retrain_if_exists=retrain, refit='neg_mean_squared_error')\n",
    "    \n",
    "    X_test_imputed[:, idx] = col_to_insert\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'XGBRegressor')\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"path = '../../../models/impute/miss_forest_imputer.pkl'\\nclf = RandomForestClassifier(n_jobs=-1)\\nrgr = RandomForestRegressor(n_jobs=-1)\\n\\nmodel = lambda params: MissForest(clf(**params), rgr(**params))\\n\\nparameter_space = {\\n    'n_estimators': [15, 25, 35],\\n    'max_depth': [2, 3, 5, 7],\\n}\\n\\nretrain = True\\n\\n# impute the data with the miss forest imputer\\nX_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\\n                        param_space=parameter_space, scoring='neg_mean_squared_error', \\n                        path_to_save=path, normalize=False, retrain_if_exists=retrain)\\n\\n# calculate mae and mse for the imputed data for each column in the missing_idx\\nmetrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'MissForest')\\n\\nprint(metrics)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"path = '../../../models/impute/miss_forest_imputer.pkl'\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "rgr = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "model = lambda params: MissForest(clf(**params), rgr(**params))\n",
    "\n",
    "parameter_space = {\n",
    "    'n_estimators': [15, 25, 35],\n",
    "    'max_depth': [2, 3, 5, 7],\n",
    "}\n",
    "\n",
    "retrain = True\n",
    "\n",
    "# impute the data with the miss forest imputer\n",
    "X_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\n",
    "                        param_space=parameter_space, scoring='neg_mean_squared_error', \n",
    "                        path_to_save=path, normalize=False, retrain_if_exists=retrain)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'MissForest')\n",
    "\n",
    "print(metrics)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3307.973839  0.956714     27552228.6627  1.409101\n",
      "IterativeImputer  2952.872716  0.906348   24813904.064018  1.302942\n",
      "KNNImputer        2863.905205   0.88298   25430505.059991  1.429329\n",
      "XGBRegressor      2614.571848  0.801925  215684985.646753  1.116717\n",
      "MissForest        2466.242003  0.856916   20861725.111218  1.264165\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try load the miss forest imputer with pickle\n",
    "    with open('../../../models/impute/miss_forest.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "except:\n",
    "    # impute the data with the miss forest imputer\n",
    "    params = {\n",
    "        'n_estimators': 15,\n",
    "        'max_depth': 3 \n",
    "        }\n",
    "\n",
    "    clf = RandomForestClassifier(n_jobs=-1, **params)\n",
    "    rgr = RandomForestRegressor(n_jobs=-1, **params)\n",
    "    imputer = MissForest(clf, rgr)\n",
    "\n",
    "    # fit the imputer\n",
    "    X_train_df = pd.DataFrame(X_train) # convert to dataframe for MissForest\n",
    "    imputer.fit(X_train_df)\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../../models/impute/miss_forest.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test) # convert to dataframe for MissForest\n",
    "X_test_imputed = imputer.transform(X_test_df.copy())\n",
    "X_test_imputed = X_test_imputed.values # convert to numpy array\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'MissForest')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the columns of the metrics dataframe\n",
    "scaler = StandardScaler()\n",
    "metrics_scaled = scaler.fit_transform(metrics)\n",
    "\n",
    "# sum along each row to get the total error\n",
    "metrics_scaled = np.sum(metrics_scaled, axis=1)\n",
    "\n",
    "# find the indices of the n best imputers\n",
    "n = 2 # optimal number of imputers\n",
    "best_imputers_idx = metrics_scaled.argsort()[:n]\n",
    "\n",
    "# find the names of the best imputers\n",
    "best_imputers = metrics.index[best_imputers_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scaled: [ 3.54603455  0.36664074  0.7364284  -1.98465299 -2.6644507 ]\n",
      "Best imputers: Index(['MissForest', 'XGBRegressor'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Metrics scaled: {metrics_scaled}')\n",
    "print(f'Best imputers: {best_imputers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ensmble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on imputer: MissForest\n",
      "Working on imputer: XGBRegressor\n",
      "                        mae_4     mae_9             mse_4     mse_9\n",
      "SimpleImputer     3307.973839  0.956714     27552228.6627  1.409101\n",
      "IterativeImputer  2952.872716  0.906348   24813904.064018  1.302942\n",
      "KNNImputer        2863.905205   0.88298   25430505.059991  1.429329\n",
      "XGBRegressor      2614.571848  0.801925  215684985.646753  1.116717\n",
      "MissForest        2466.242003  0.856916   20861725.111218  1.264165\n",
      "Ensembled         2454.672494  0.837318   67573048.722437  1.191029\n"
     ]
    }
   ],
   "source": [
    "main_path = '../../../models/impute/'\n",
    "imputers_path = {\n",
    "    'SimpleImputer': main_path + 'simple_imputer.pkl',\n",
    "    'IterativeImputer': main_path + 'iterative_imputer.pkl',\n",
    "    'KNNImputer': main_path + 'knn_imputer.pkl',\n",
    "    'XGBRegressor': main_path + 'xgb_imputer',\n",
    "    'MissForest': main_path + 'miss_forest.pkl'\n",
    "}\n",
    "\n",
    "# try opnening the ensembled imputed data\n",
    "try:\n",
    "    with open('../../../data/processed/impute_test/ensembled_imputed.pkl', 'rb') as f:\n",
    "        X_test_imputed = pkl.load(f)\n",
    "except:\n",
    "    # create the ensembled imputed data\n",
    "    X_test_imputed = np.zeros(X_test.shape)\n",
    "\n",
    "    for imputer_name in best_imputers:\n",
    "        print('Working on imputer:', imputer_name)\n",
    "        \n",
    "        if imputer_name != 'XGBRegressor':\n",
    "            with open(imputers_path[imputer_name], 'rb') as f:\n",
    "                imputer = pkl.load(f)\n",
    "\n",
    "            if imputer_name == 'MissForest':\n",
    "                X_test_df = pd.DataFrame(X_test)\n",
    "                imputed = imputer.transform(X_test_df).values\n",
    "\n",
    "            else:\n",
    "                imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "            # set values to 0 if they are negative\n",
    "            imputed[imputed < 0] = 0\n",
    "\n",
    "            X_test_imputed += imputed\n",
    "\n",
    "        else:\n",
    "            for idx in missing_idx:\n",
    "                with open(imputers_path[imputer_name] + f'_{idx}.pkl', 'rb') as f:\n",
    "                    imputer = pkl.load(f)\n",
    "\n",
    "                y_test_i = np.copy(y_test[:, idx])\n",
    "                X_test_i = np.delete(np.copy(X_test), idx, axis=1)\n",
    "\n",
    "                imputed = imputer.predict(X_test_i)\n",
    "                imputed[imputed < 0] = 0\n",
    "                X_test_imputed[:, idx] += imputed\n",
    "\n",
    "    X_test_imputed /= n\n",
    "\n",
    "    # save the ensembled imputed data\n",
    "    with open('../../../data/processed/impute_test/ensembled_imputed.pkl', 'wb') as f:\n",
    "        pkl.dump(X_test_imputed, f)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'Ensembled')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on imputer: MissForest\n",
      "Working on imputer: XGBRegressor\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try read the data\n",
    "    train_df = pd.read_csv('../../../data/proessed/imputed_data/train_imputed.csv')\n",
    "    test_df = pd.read_csv('../../../data/processed/imputed_data/test_imputed.csv')\n",
    "\n",
    "except:\n",
    "    data_to_impute = data.copy()\n",
    "\n",
    "    # remove the columns\n",
    "    columns_to_drop = ['Unnamed: 0', 'SeriousDlqin2yrs']\n",
    "    data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    columns_of_data = data_to_impute.columns\n",
    "\n",
    "    # create an array to store the imputed data\n",
    "    imputed_data = np.copy(data_to_impute.values)\n",
    "    missing_value_mask = (data_to_impute).isna()\n",
    "    missing_value_mask = missing_value_mask.values\n",
    "\n",
    "    # set all values to zero\n",
    "    imputed_data[missing_value_mask] = 0\n",
    "\n",
    "    # impute the data with the best imputers\n",
    "    for imputer_name in best_imputers:\n",
    "        print('Working on imputer:', imputer_name)\n",
    "        \n",
    "        if imputer_name != 'XGBRegressor':\n",
    "            with open(imputers_path[imputer_name], 'rb') as f:\n",
    "                imputer = pkl.load(f)\n",
    "\n",
    "            if imputer_name == 'MissForest':\n",
    "                data_to_impute = pd.DataFrame(np.copy(data_to_impute))\n",
    "                imputed = imputer.transform(data_to_impute).values\n",
    "\n",
    "            else:\n",
    "                imputed = imputer.transform(data_to_impute.copy())\n",
    "\n",
    "            # set values to 0 if they are negative\n",
    "            imputed[imputed < 0] = 0\n",
    "\n",
    "            imputed_data[missing_value_mask] += imputed[missing_value_mask]\n",
    "\n",
    "        else:\n",
    "            for idx in missing_idx:\n",
    "                with open(imputers_path[imputer_name] + f'_{idx}.pkl', 'rb') as f:\n",
    "                    imputer = pkl.load(f)\n",
    "                \n",
    "                X = np.delete(np.copy(data_to_impute), idx, axis=1)\n",
    "                imputed = imputer.predict(X)\n",
    "                imputed[imputed < 0] = 0\n",
    "                xgb_mask_i = missing_value_mask[:, idx]\n",
    "                xgb_mask = missing_value_mask & (np.arange(data_to_impute.shape[1]) == idx)\n",
    "                imputed_data[xgb_mask] += imputed[xgb_mask_i]\n",
    "\n",
    "    imputed_data[missing_value_mask] /= n\n",
    "\n",
    "    # make the imputed data a dataframe with the same columns as the original data\n",
    "    imputed_data = pd.DataFrame(imputed_data, columns=columns_of_data)\n",
    "\n",
    "    # add back the columns that were dropped\n",
    "    imputed_data['Unnamed: 0'] = data['Unnamed: 0']\n",
    "    imputed_data['SeriousDlqin2yrs'] = data['SeriousDlqin2yrs']\n",
    "\n",
    "    # split back into train and test\n",
    "    train_df = imputed_data.iloc[:ntrain, :]\n",
    "    test_df = imputed_data.iloc[ntrain:, :]\n",
    "\n",
    "    # save the data\n",
    "    train_df.to_csv('../../../data/processed/imputed_data/train_imputed.csv', index=False)\n",
    "    test_df.to_csv('../../../data/processed/imputed_data/test_imputed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from missforest.missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from impute import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data\n",
    "train_df = pd.read_csv('../../../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../../../data/raw/test.csv')\n",
    "\n",
    "# save the length of the train data\n",
    "ntrain = train_df.shape[0]\n",
    "\n",
    "# concatenate the data\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>7959.688894</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165166</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>2851.722407</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642979</td>\n",
       "      <td>1115.657341</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3603.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0           0               0.0                              0.034949  59.0   \n",
       "1           1               0.0                              0.155308  47.0   \n",
       "2           2               0.0                              0.165166  62.0   \n",
       "3           3               0.0                              0.010886  61.0   \n",
       "4           4               0.0                              0.000717  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                   0.0     0.004933    7959.688894   \n",
       "1                                   0.0   881.000000            NaN   \n",
       "2                                   1.0     0.020327    2851.722407   \n",
       "3                                   0.0     0.642979    1115.657341   \n",
       "4                                   0.0  3603.000000            NaN   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                              5.0                      0.0   \n",
       "1                              6.0                      0.0   \n",
       "2                              8.0                      0.0   \n",
       "3                              6.0                      0.0   \n",
       "4                             15.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           0.0                                   0.0   \n",
       "1                           1.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           1.0                                   0.0   \n",
       "4                           3.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                  0\n",
       "SeriousDlqin2yrs                        37500\n",
       "RevolvingUtilizationOfUnsecuredLines        0\n",
       "age                                         0\n",
       "NumberOfTime30-59DaysPastDueNotWorse        0\n",
       "DebtRatio                                   0\n",
       "MonthlyIncome                           29731\n",
       "NumberOfOpenCreditLinesAndLoans             0\n",
       "NumberOfTimes90DaysLate                     0\n",
       "NumberRealEstateLoansOrLines                0\n",
       "NumberOfTime60-89DaysPastDueNotWorse        0\n",
       "NumberOfDependents                       3924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                              112500\n",
       "SeriousDlqin2yrs                             2\n",
       "RevolvingUtilizationOfUnsecuredLines    125728\n",
       "age                                         89\n",
       "NumberOfTime30-59DaysPastDueNotWorse        16\n",
       "DebtRatio                               114194\n",
       "MonthlyIncome                           118636\n",
       "NumberOfOpenCreditLinesAndLoans             58\n",
       "NumberOfTimes90DaysLate                     19\n",
       "NumberRealEstateLoansOrLines                28\n",
       "NumberOfTime60-89DaysPastDueNotWorse        13\n",
       "NumberOfDependents                          13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the unique values for each column\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute = data.copy()\n",
    "\n",
    "# remove the columns\n",
    "columns_to_drop = ['Unnamed: 0', 'SeriousDlqin2yrs']\n",
    "data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "length_of_dataset = data_to_impute.shape[0]\n",
    "\n",
    "# calculate number of missing values by column\n",
    "missing_values_by_column = data_to_impute.isna().sum()\n",
    "prc_missing_values_by_column = missing_values_by_column / length_of_dataset\n",
    "\n",
    "# drop missing values\n",
    "data_to_impute.dropna(inplace=True)\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test = train_test_split(data_to_impute, test_size=0.2, random_state=0) # 80% train, 20% test both with no missing values\n",
    "X_train, X_test = X_train.values, X_test.values # convert to numpy arrays\n",
    "y_train, y_test = np.copy(X_train), np.copy(X_test) # copy the test data to use as the ground truth\n",
    "\n",
    "# assert there are np missing values in any of the datasets\n",
    "assert np.isnan(X_train).sum() == 0\n",
    "assert np.isnan(X_test).sum() == 0\n",
    "assert np.isnan(y_train).sum() == 0\n",
    "assert np.isnan(y_test).sum() == 0\n",
    "\n",
    "del data, data_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_values(X, prc_missing_values_by_column):\n",
    "  \n",
    "    # create a boolean array of the size of the data\n",
    "    missing_value_mask = np.zeros(X.shape, dtype=bool)\n",
    "\n",
    "    for i in range(X_test.shape[1]):\n",
    "        prc_missing = prc_missing_values_by_column[i]\n",
    "\n",
    "        # calculate how many missing values to create\n",
    "        n_to_nan = int(prc_missing * X_test.shape[0])\n",
    "\n",
    "        # randomly select n_missing indexes for the column\n",
    "        idx = np.random.choice(X_test.shape[0], n_to_nan, replace=False)\n",
    "        missing_value_mask[idx, i] = True\n",
    "\n",
    "    # create the missing values\n",
    "    X[missing_value_mask] = np.nan\n",
    "\n",
    "    return X, missing_value_mask\n",
    "\n",
    "# create missing values in the train data\n",
    "X_train, missing_value_mask_train = create_missing_values(X_train.copy(), prc_missing_values_by_column)\n",
    "\n",
    "# create missing values in the test data\n",
    "X_test, missing_value_mask_test = create_missing_values(X_test.copy(), prc_missing_values_by_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(metrics_df, X_test_imputed, y, mask, model_name):\n",
    "    \"\"\"\n",
    "    Update the metrics dataframe with Mean Absolute Error (MAE) and Mean Squared Error (MSE) for each missing index.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_df (pandas.DataFrame): The metrics dataframe to update.\n",
    "    - X_test_imputed (numpy.ndarray): The imputed test data.\n",
    "    - y (numpy.ndarray): The ground truth test data.\n",
    "    - mask (numpy.ndarray): The mask indicating missing values.\n",
    "    - model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df (pandas.DataFrame): The updated metrics dataframe.\n",
    "    \"\"\"\n",
    "    missing_idx = np.unique(np.where(mask)[1])\n",
    "    for i in missing_idx:\n",
    "        mae = np.mean(np.abs(X_test_imputed[mask[:, i], i] - y[mask[:, i], i]))\n",
    "        mse = np.mean((X_test_imputed[mask[:, i], i] - y[mask[:, i], i])**2)\n",
    "\n",
    "        metrics_df.loc[model_name, f'mae_{i}'] = mae\n",
    "        metrics_df.loc[model_name, f'mse_{i}'] = mse\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer  3263.309853  0.900387  35972760.408643  1.266904\n"
     ]
    }
   ],
   "source": [
    "# impute the data with the simple imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(X_train.copy())\n",
    "\n",
    "# create a dataframe to store the metrics\n",
    "missing_idx = np.unique(np.where(missing_value_mask_test)[1])\n",
    "columns_of_df = [f'mae_{i}' for i in missing_idx] + [f'mse_{i}' for i in missing_idx]\n",
    "metrics = pd.DataFrame(columns=columns_of_df)\n",
    "\n",
    "# impute the data\n",
    "X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'SimpleImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_prc_mae(estimator, X, y, parameters):\n",
    "\n",
    "    y_pred = estimator.set_params(**parameters).transform(X.copy())\n",
    "\n",
    "    # calculate the mae for each column\n",
    "    mae = np.mean(np.abs(y_pred - y))\n",
    "\n",
    "    # calculate prc_mae for each column\n",
    "    prc_mae = mae / np.mean(np.abs(y))\n",
    "\n",
    "    # sum the prc_mae for each column\n",
    "    prc_mae_sum = np.sum(prc_mae)\n",
    "\n",
    "    return -prc_mae_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3263.309853  0.900387  35972760.408643  1.266904\n",
      "IterativeImputer  2888.988649  0.864046  32481200.799444  1.196151\n"
     ]
    }
   ],
   "source": [
    "# define the path to save the model\n",
    "path_to_save = '../../../models/impute/iterative_imputer.pkl'\n",
    "\n",
    "model = IterativeImputer(random_state=0)\n",
    "\n",
    "parameter_space = {\n",
    "    'max_iter': [25, 50, 75],\n",
    "    'n_nearest_features': [5, 10, 15],\n",
    "    'initial_strategy': ['mean', 'median', 'most_frequent'],\n",
    "}\n",
    "\n",
    "retrain = False\n",
    "\n",
    "# scoring function, additional variables added only for compatibility with the scoring function\n",
    "scoring = lambda estimator, X, y, **param: neg_prc_mae(estimator, X, y, param)\n",
    "\n",
    "# impute the data with the iterative imputer\n",
    "X_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\n",
    "                        param_space=parameter_space, scoring=scoring, \n",
    "                        path_to_save=path_to_save, normalize=False, retrain_if_exists=retrain)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'IterativeImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the imputer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Best Score: \u001b[1m-0.027179624756817383\u001b[0m\n",
      "Best Hyperparameters:\n",
      "metric: nan_euclidean\n",
      "n_neighbors: 5\n",
      "weights: uniform\n",
      "\n",
      "Fine tuning completed.\n",
      "Imputer saved.\n",
      "Imputing the test data...\n",
      "Imputation completed.\n",
      "                        mae_4     mae_9            mse_4     mse_9\n",
      "SimpleImputer     3263.309853  0.900387  35972760.408643  1.266904\n",
      "IterativeImputer  2888.988649  0.864046  32481200.799444  1.196151\n",
      "KNNImputer        2982.612897   0.82194  61485710.999823   1.27345\n"
     ]
    }
   ],
   "source": [
    "# impute the data with knn imputer\n",
    "path = '../../../models/impute/knn_imputer.pkl'\n",
    "path_data = '../../../data/processed/impute_test/knn_imputed.pkl'\n",
    "\n",
    "model = KNNImputer()\n",
    "\n",
    "parameter_space = {\n",
    "    'n_neighbors': [5],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['nan_euclidean', 'nan_manhattan', 'nan_chebyshev', 'nan_minkowski'],\n",
    "}\n",
    "\n",
    "# scoring function, additional variables added only for compatibility with the scoring function\n",
    "scoring = lambda estimator, X, y, **param: neg_prc_mae(estimator, X, y.copy(), param)\n",
    "\n",
    "retrain = True\n",
    "\n",
    "# impute the data with the knn imputer\n",
    "X_test_imputed = impute(X_train.copy(), X_test.copy(), y_train.copy(), model,\n",
    "                        param_space=parameter_space, scoring=scoring, \n",
    "                        path_to_save=path, path_data=path_data, normalize=False, retrain_if_exists=retrain)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "metrics = update_metrics(metrics, X_test_imputed, y_test, missing_value_mask_test, 'KNNImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # try load the knn imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "    # try load the imputed data\n",
    "    with open('../../data/processed/impute/knn_imputed.pkl', 'rb') as f:\n",
    "        X_test_imputed = pkl.load(f)\n",
    "except:\n",
    "    # impute the data with the knn imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # impute the data\n",
    "    X_test_imputed = imputer.transform(X_test.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/knn_imputer.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "    # save the imputed data\n",
    "    with open('../../data/processed/impute/knn_imputed.pkl', 'wb') as f:\n",
    "        pkl.dump(X_test_imputed, f)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'KNNImputer')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(X, y, space, scoring, \n",
    "         model, modeltype='clf', search_type='grid', n_iter_random=100,\n",
    "         n_splits=5, n_repeats=3, random_state=1,\n",
    "         verbose=True, display_plots=False):\n",
    "    \n",
    "    # define evaluation\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "  \n",
    "    if verbose:\n",
    "        verbosity = 1\n",
    "\n",
    "    # define search\n",
    "    if search_type == 'grid':\n",
    "        search = GridSearchCV(model, space, scoring=scoring, n_jobs=-1, cv=cv, verbose=verbosity)\n",
    "    elif search_type == 'random':\n",
    "        search = RandomizedSearchCV(model, space, scoring=scoring, n_jobs=-1, cv=cv, n_iter=n_iter_random, verbose=verbosity)\n",
    "    \n",
    "    # execute search\n",
    "    result = search.fit(X, y)\n",
    "    \n",
    "    # plot results\n",
    "    if display_plots:\n",
    "        results_df = pd.DataFrame(result.cv_results_)\n",
    "        for key, values in space.items():\n",
    "            \n",
    "            # group the results by the hyperparameter\n",
    "            param_means = []\n",
    "            param_stds = []\n",
    "            for value in values:\n",
    "                mask = results_df['param_' + key] == value\n",
    "                param_means.append(np.mean(results_df[mask]['mean_test_score']))\n",
    "                param_stds.append(np.std(results_df[mask]['mean_test_score']))\n",
    "            \n",
    "            # create plot with two subplots side by side\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            fig.suptitle(key)\n",
    "            ax[0].plot(values, param_means)\n",
    "            ax[0].set_title('Mean test scores')\n",
    "            ax[0].set_xlabel(key)\n",
    "            ax[0].set_ylabel('mean scores')\n",
    "            padding = 0.1\n",
    "            ax[0].set_ylim(max(0, min(param_means) - padding), min(1, max(param_means) + padding))\n",
    "\n",
    "            ax[1].plot(values, param_stds)\n",
    "            ax[1].set_title('Mean score std')\n",
    "            ax[1].set_xlabel(key)\n",
    "            ax[1].set_ylabel('score std')\n",
    "            padding = 0.05\n",
    "            ax[1].set_ylim(max(0, min(param_stds) - padding), min(1, max(param_stds) + padding))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    # summarize result\n",
    "    if verbose:\n",
    "        print('Best Score: %s' % result.best_score_)\n",
    "        print('Best Hyperparameters:')\n",
    "        for k, v in result.best_params_.items():\n",
    "            print('%s: %s' % (k, v))\n",
    "\n",
    "    # best model\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result.best_params_, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices of the columns with missing values\n",
    "missing_idx = np.where(np.sum(mask, axis=0) > 0)[0]\n",
    "\n",
    "# define the space of hyperparameters\n",
    "param_space = {\n",
    "    'n_estimators': [150, 250],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1],\n",
    "    'colsample_bytree': [0.6, 0.8, 1],\n",
    "    'colsample_bylevel': [0.6, 0.8, 1]\n",
    "}\n",
    "\n",
    "X_test_imputed = np.copy(X_test)\n",
    "\n",
    "for idx in missing_idx:\n",
    "    print(f'Imputing column {idx}...')\n",
    "    X_train_xgboost = np.copy(X_train)\n",
    "    X_train_xgboost = np.delete(X_train, idx, axis=1)\n",
    "    y_train_xgboost = np.copy(X_train[:, idx])\n",
    "\n",
    "    X_test_xgboost = np.copy(X_test)\n",
    "    X_test_xgboost = np.delete(X_test, idx, axis=1)\n",
    "    y_test_xgboost = np.copy(X_test[:, idx])\n",
    "\n",
    "    # find the the indices of the missing values in the column\n",
    "    nan_mask = np.isnan(y_test_xgboost)\n",
    "\n",
    "    try:\n",
    "        # try load the model with pickle\n",
    "        with open(f'../../models/impute/xgboost_{idx}.pkl', 'rb') as f:\n",
    "            best_model = pkl.load(f)\n",
    "    except:\n",
    "        # tune the model\n",
    "        best_params, best_model = tune(X=X_train_xgboost, y=y_train_xgboost, space=param_space, \n",
    "                                       scoring='neg_mean_squared_error', model=XGBRegressor(), \n",
    "                                       modeltype='reg', search_type='grid', n_iter_random=100, \n",
    "                                       n_splits=3, n_repeats=1, random_state=1, verbose=True, display_plots=False)\n",
    "\n",
    "        # save the model with pickle\n",
    "        with open(f'../../models/impute/xgboost_{idx}.pkl', 'wb') as f:\n",
    "            pkl.dump(best_model, f)\n",
    "\n",
    "    # impute the data with the xgboost imputer\n",
    "    y_pred = best_model.predict(X_test_xgboost)\n",
    "\n",
    "    # replace the missing values\n",
    "    X_test_imputed[nan_mask, idx] = y_pred[nan_mask]\n",
    "\n",
    "    print('')\n",
    "\n",
    "# for each column in the missing_idx calculate the mae and mse\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'XGBoost')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the miss forest imputer\n",
    "\n",
    "try:\n",
    "    # try load the miss forest imputer with pickle\n",
    "    with open('../../models/impute/miss_forest.pkl', 'rb') as f:\n",
    "        imputer = pkl.load(f)\n",
    "\n",
    "except:\n",
    "    # impute the data with the miss forest imputer\n",
    "    clf = RandomForestClassifier(n_jobs=-1)\n",
    "    rgr = RandomForestRegressor(n_jobs=-1)\n",
    "    imputer = MissForest(clf, rgr, max_iter=1000000)\n",
    "\n",
    "    # fit the imputer\n",
    "    imputer.fit(X_train.copy())\n",
    "\n",
    "    # save the imputer with pickle\n",
    "    with open('../../models/impute/miss_forest.pkl', 'wb') as f:\n",
    "        pkl.dump(imputer, f)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "X_test_imputed = imputer.transform(X_test_df.copy())\n",
    "X_test_imputed = X_test_imputed.values\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'MissForest')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the columns of the metrics dataframe\n",
    "scaler = StandardScaler()\n",
    "metrics_scaled = scaler.fit_transform(metrics)\n",
    "\n",
    "# sum along each row to get the total error\n",
    "metrics_scaled = np.sum(metrics_scaled, axis=1)\n",
    "\n",
    "# find the indices of the n best imputers\n",
    "n = 3\n",
    "best_imputers_idx = metrics_scaled.argsort()[:n]\n",
    "\n",
    "# find the names of the best imputers\n",
    "best_imputers = metrics.index[best_imputers_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Metrics scaled: {metrics_scaled}')\n",
    "print(f'Best imputers: {best_imputers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers_dict = {\n",
    "    'SimpleImputer': SimpleImputer(strategy='mean'),\n",
    "    'IterativeImputer': IterativeImputer(max_iter=1000, random_state=0),\n",
    "    'KNNImputer': KNNImputer(n_neighbors=5),\n",
    "    'MissForest': MissForest(clf, rgr, max_iter=1000000)\n",
    "}\n",
    "\n",
    "try:\n",
    "    # try load the ensembled imputed data\n",
    "    with open('../../data/processed/impute/ensemble_imputed.pkl', 'rb') as f:\n",
    "        X_test_imputed = pkl.load(f)\n",
    "except:\n",
    "    X_test_imputed = np.copy(X_test)\n",
    "    mask = np.isnan(X_test)\n",
    "    X_test_imputed[mask] = 0\n",
    "\n",
    "    for imputer_name in best_imputers:\n",
    "        print(f'Imputing with {imputer_name}...')\n",
    "        \n",
    "        imputer = imputers_dict[imputer_name]\n",
    "\n",
    "        # fit the imputer\n",
    "        imputer.fit(X_train.copy())\n",
    "        print('Imputer fitted...')\n",
    "\n",
    "        # impute the data\n",
    "        try:\n",
    "            data_to_add = imputer.transform(np.copy(X_test))\n",
    "        except:\n",
    "            data_to_add = imputer.transform(pd.DataFrame(np.copy(X_test)))\n",
    "            data_to_add = data_to_add.values\n",
    "        print('Data imputed...')\n",
    "\n",
    "        # add the imputed data to the imputed_X_test\n",
    "        X_test_imputed[mask] += data_to_add[mask]\n",
    "        print('')\n",
    "\n",
    "    # average the imputed data\n",
    "    X_test_imputed[mask] /= n\n",
    "\n",
    "    # save the imputed data as a pickle\n",
    "    with open('../../data/processed/impute/ensemble_imputed.pkl', 'wb') as f:\n",
    "        pkl.dump(X_test_imputed, f)\n",
    "\n",
    "# calculate mae and mse for the imputed data for each column in the missing_idx\n",
    "update_metrics(metrics, missing_idx, X_test_imputed, y, mask, 'Ensemble')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # try read the data\n",
    "    train_df = pd.read_csv('../../data/proessed/train_imputed.csv')\n",
    "    test_df = pd.read_csv('../../data/processed/test_imputed.csv')\n",
    "except:\n",
    "    data_to_impute = data.copy()\n",
    "\n",
    "    # remove the columns\n",
    "    data_to_impute.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    mask = data_to_impute.isna()\n",
    "\n",
    "    imputed_data = np.copy(data_to_impute.values)\n",
    "\n",
    "    # set all values to zero\n",
    "    imputed_data[mask] = 0\n",
    "\n",
    "    # impute the data with the best imputers\n",
    "    for imputer_name in best_imputers:\n",
    "        print(f'Imputing with {imputer_name}...')\n",
    "        \n",
    "        imputer = imputers_dict[imputer_name]\n",
    "\n",
    "        # fit the imputer\n",
    "        imputer.fit(data_to_impute.copy())\n",
    "        print('Imputer fitted...')\n",
    "\n",
    "        # impute the data\n",
    "        try:\n",
    "            data_to_add = imputer.transform(data_to_impute.copy())\n",
    "            data_to_add = data_to_add.values\n",
    "        except:\n",
    "            data_to_add = imputer.transform(data_to_impute.copy().values)\n",
    "        print('Data imputed...')\n",
    "\n",
    "        # add the imputed data to the imputed_X_test\n",
    "        imputed_data[mask] += data_to_add[mask]\n",
    "        print('')\n",
    "\n",
    "    # average the imputed data\n",
    "    imputed_data[mask] /= n\n",
    "\n",
    "    # make the imputed data a dataframe with the same columns as the original data\n",
    "    imputed_data = pd.DataFrame(imputed_data, columns=data_to_impute.columns)\n",
    "\n",
    "    # add back the columns that were dropped\n",
    "    imputed_data['Unnamed: 0'] = data['Unnamed: 0']\n",
    "    imputed_data['SeriousDlqin2yrs'] = data['SeriousDlqin2yrs']\n",
    "\n",
    "    # split back into train and test\n",
    "    train_df = imputed_data.iloc[:ntrain, :]\n",
    "    test_df = imputed_data.iloc[ntrain:, :]\n",
    "\n",
    "    # save the data\n",
    "    train_df.to_csv('../../data/processed/imputed_data/train_imputed.csv', index=False)\n",
    "    test_df.to_csv('../../data/processed/imputed_data/test_imputed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
